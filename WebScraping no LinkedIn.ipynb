{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d51dcc-b777-46a0-b0c4-7638c82fc70a",
   "metadata": {},
   "source": [
    "# Executando um Processo de WebScraping para Extrair Informações da Rede Social LinkedIn\n",
    "\n",
    "O Web Scraping nada mais é que uma extração de dados na internet de forma automatizada, amplamente usada ao redor do mundo para capturar todo tipo de informação relevante para tomadas de decisões, direcionamento de marketing, estudo de concorrência pessoal ou empresarial, desenvolver sistemas de recomendação de produtos ou serviços, e para diversas outras finalidades. \n",
    "\n",
    "As possibilidades e o poder desse processo são quase ilimitados, onde basicamente tudo que é visível na Web é possível ser extraído, e portanto o famoso conselho do Tio Ben no filme do Homem-Aranha (2002) se aplica tão bem ao Web Scraping:\n",
    "\n",
    "<center><img alt=\"Famosa frase do Tio Ben\" src='https://pm1.narvii.com/6409/5097b0d352a5cb408028a0208de49116efaedd37_hq.jpg'></center>\n",
    "\n",
    "Fonte: Amino.\n",
    "\n",
    "Todos os dias deixamos rastros e informações pessoais visíveis na internet, seja pelos mecanismos de buscas ou principalmente pelas redes sociais, que para muitas pessoas é um local de compartilhar seus momentos de vida, ou melhor, compartilhar seus dados para todas as pessoas na rede, que para aqueles que possuem intenções maliciosas é um prato cheio ter acesso a essas informações. Por isso, a frase do Tio Ben é tão importante para o Web Scraping, da mesma forma que podem ser desenvolvidas aplicações incríveis, também podem ser desenvolvidas aplicações maliciosas (por exemplo: venda de informações pessoais).\n",
    "\n",
    "Como a maioria das grandes organizações reconhecem o valor dessas informações de seus usuários, políticas contra o uso do Web Scraping foram desenvolvidas para tentar controlar o que pode, ou não, ser extraído de seus produtos, aplicando penalidades para aqueles que quebrem essas políticas. Além disso, muitas delas vêm dificultando o processo de Scraping nos últimos anos com certa frequência, como é o caso do LinkedIn, que será a rede social que iremos abordar nesse projeto.\n",
    "\n",
    "Para aqueles que tentaram realizar o processo de Web Scraping no LinkedIn, provavelmente ficaram conhecendo algumas das lutas dessa rede profissional para proibir o uso de Scraping para extrair dados de seus usuários, sendo o caso mais conhecido com a hiQ Labs, que possui uma história de quase 6 anos na justiça.\n",
    "\n",
    "A ação judicial teve início em 2017, quando o LinkedIn acusou a hiQ Lab de coletar dados de usuários da rede social de forma abusiva, onde a hiQ Labs estaria usando as informações dos usuários para desenvolver algoritmos que poderiam prever quando os funcionários poderiam deixar seus empregos, porém o LinkedIn sofreu uma derrota quando a concorrente processou a rede afirmando que os dados públicos são de fato públicos e que a rede LinkedIn não poderia impedir que qualquer empresa possa ter acesso a esses dados. Entretanto, recentemente, outra ação foi levantada pelo LinkedIn que afirma que os Bots da hiQ Labs estariam filtrando e coletandos dados que nenhum humano conseguiria coletar, ou seja, dados não públicos, que ainda estariam sendo colocados à venda por trás dos panos.\n",
    "\n",
    "Não entrarei mais a fundo sobre essa luta judicial por não ser o foco deste projeto, para isso separei alguns artigos sobre o assunto na Seção Referências ao fim do Projeto, mas já adianto uma coisa para despertar seu interesse em lê-los: o LinkedIn ganhou a disputa judicial recentemente!\n",
    "\n",
    "O problema que iremos simular neste projeto para que o Web Scraping seja a solução é:\n",
    "\n",
    "\"***O Setor de Recursos Humanos da empresa divulga as vagas de emprego disponíveis no LinkedIn e consegue ter acesso às páginas dos membros que aplicaram para essas vagas, porém acessar cada página uma por uma para encontrar o melhor candidato é algo trabalhoso e que demanda bastante tempo, ainda mais quando a vaga recebe mais de 200 aplicações (o que é bastante comum vermos até maiores), posto essa dificuldade, será que é possível obter as informações de todas essas pessoas que aplicaram para as vagas de maneira mais prática, como, por exemplo, em uma única Tabela?***\"\n",
    "\n",
    "O objetivo deste projeto é desenvolver um processo de Web Scraping no LinkedIn para extrair algumas informações da página de um membro da rede e salvar as informações em um Pandas DataFrame que solucionará essa dor da equipe de RH do negócio, e para isso utilizaremos apenas duas bibliotecas: **Selenium** e **BeautifulSoup**.\n",
    "\n",
    "Como comentei anteriormente, o LinkedIn vem tomando medidas protetivas contra extrações de dados na Plataforma com uma certa frequência, portanto o código utilizado aqui pode não ser funcional em questão de dias, além disso tenha em mente que precisamos de uma conta ativa na plataforma para que possamos extrair os dados dos membros, na qual pode ter Features que venham a ser bloqueadas ou até mesmo suspensão total da conta, então tome cuidado para não abusar nas extrações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7aba9-3a6b-4245-a4ea-0d31a8d62153",
   "metadata": {},
   "source": [
    "## Preparação do Processo\n",
    "\n",
    "Primeiramente é preciso fazer o download do [WebDriver](https://chromedriver.chromium.org/downloads), que é responsável por realizar o processo automatizado do Web Scraping, e após a instalação importamos as bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7042a3d7-8709-465c-a9d2-dc06e9e15690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "!pip install selenium -q\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "!pip install beautifulsoup4 -q\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "!pip install pandas -q\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a2399-3f2d-45e5-a137-77d222780143",
   "metadata": {},
   "source": [
    "## Instanciando o WebDriver\n",
    "\n",
    "Para criar a instância do WebDriver é necessário passar o caminho no qual ele foi instalado, no meu caso na própria pasta do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f15b11-a442-473d-94ed-174959426503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a instância no WebDriver\n",
    "srv = Service('CAMINHO DO WEB DRIVER\\chromedriver.exe') \n",
    "opt = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=srv, options=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa6200-8318-4df9-8315-f8dd9f19c5c1",
   "metadata": {},
   "source": [
    "## Logando em sua Conta no LinkedIn\n",
    "\n",
    "Aqui iremos acessar a página de Login da Plataforma e passar os dados da conta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac213b9-fce8-49f4-83b5-a71caac3f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo a página de Login do LinkedIn\n",
    "driver.get('https://www.linkedin.com/login/')\n",
    "time.sleep(1) # aguardar 1 segundos para a página abrir\n",
    "\n",
    "# Inserindo o usuário\n",
    "usuario = driver.find_element('id','username') # acessando o campo usuário\n",
    "usuario.send_keys('SEU LOGIN') # enviando a string com o usuário\n",
    "\n",
    "# Inserindo a senha\n",
    "senha = driver.find_element('id','password') # acessando o campo senha\n",
    "senha.send_keys('SUA SENHA') # enviando a string com a senha\n",
    "\n",
    "# Clicando no botão 'Entrar' para acessar a conta no LinkedIn\n",
    "driver.find_element(\"xpath\",\"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0da3a-e9a5-4f16-bdb9-3b0578d4ca7f",
   "metadata": {},
   "source": [
    "## Acessando um Perfil através da URL do Usuário\n",
    "\n",
    "Como descrito em nossa situação problema, temos as URLs dos candidatos que aplicaram às vagas postadas no LinkedIn pela equipe de RH do negócio, mas por enquanto vamos testar nosso processo de Web Scraping para apenas um único candidato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9d84b0-6ad3-4fe4-8362-416ae1ada75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando o perfil\n",
    "perfil_url = 'https://www.linkedin.com/in/crisjunqueira/' # ou 'https://www.linkedin.com/in/williamhgates/'\n",
    "driver.get(perfil_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3461e-fa16-4108-b1f2-e146410f318e",
   "metadata": {},
   "source": [
    "## Explorando uma Feature do LinkedIn (Baixar Perfil em PDF)\n",
    "\n",
    "Uma Feature interessante do LinkedIn é a possibilidade de baixar as informações do perfil em um formato de currículo em PDF, então como um extra adicionaremos essa funcionalidade ao nosso Scraping, ou seja, para cada perfil teremos um 'Currículo' em PDF que pode ser explorado com outras ferramentas de extração de dados em documentos.\n",
    "\n",
    "Alguns perfis possuem diferentes layouts de página, portanto veremos várias partes do código com `TRY`/`EXCEPT` para buscar generalizar o código para mais formatos de página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9771fd7a-3571-429d-b088-0f193e94fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "    driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/button').click()\n",
    "    time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "    # Clicando no botão \"Salvar como PDF\"\n",
    "    driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/div/div/ul/li[3]/div').click()\n",
    "\n",
    "except:\n",
    "    # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "    driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/button').click()\n",
    "    time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "    # Clicando no botão \"Salvar como PDF\"\n",
    "    driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/div/div/ul/li[3]/div').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf4a1c-22f8-4fcd-afa2-29ddd9fcde7c",
   "metadata": {},
   "source": [
    "## Quais Informações devemos extrair?\n",
    "\n",
    "Voltando a nossa situação problema, digamos que a equipe de RH nos passou uma lista de quais informações são chaves para a tomada de decisão sobre o candidato, que são:\n",
    "\n",
    "* Nome Candidato;\n",
    "* Título do Candidato;\n",
    "* Organização Atual;\n",
    "* Localização do Candidato;\n",
    "* Biografia;\n",
    "* Título da Última Experiência Profissional;\n",
    "* Nome da Organização da Última Experiência Profissional;\n",
    "* Tipo de Contratação da Organização;\n",
    "* Data de Início da Experiência;\n",
    "* Data de Término da Experiência;\n",
    "* Duração da Experiência;\n",
    "* Instituição referente Última Formação Acadêmica;\n",
    "* Tipo da Formação Acadêmica;\n",
    "* Título da Formação Acadêmica;\n",
    "* Duração da Formação Acadêmica;\n",
    "* Número de Seguidores;\n",
    "* Idiomas;\n",
    "* Prêmios e Honrarias;\n",
    "* Quantidade de Recomendações.\n",
    "\n",
    "Como um extra para gestão de base de dados, vamos acrescentar as seguintes informações:\n",
    "* Data de Ingestão (quando os perfis foram inseridos na base);\n",
    "* Disponibilidade da URL para consultas manuais (caso o recrutador queira acessar o perfil do candidato);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b492d26-e0b4-4a94-8e37-8e2ca0333ce8",
   "metadata": {},
   "source": [
    "## Obtendo o Código Fonte HTML da Página\n",
    "\n",
    "Antes de extrairmos as informações, precisamos obter o código fonte da página em HTML e para isso iremos fazer com que o Driver desça até o fim da página e depois criaremos um Objeto com a biblioteca **BeautifulSoup**, no qual usaremos para poder acessar as informações no código fonte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30677209-d694-4f46-90bb-1ca14f9f1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a22fac-d230-4cff-81ea-16ac776b9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o código fonte da página em uma variável\n",
    "src = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup = BeautifulSoup(src,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79992d19-33a2-44a0-a004-43496611107d",
   "metadata": {},
   "source": [
    "## Extraindo as Informações da Introdução do Perfil\n",
    "\n",
    "<center><img alt=\"Exemplo de Introdução no LinkedIn\" src='https://github.com/octavianosilva/Imagens_projetos/blob/main/Introdu%C3%A7%C3%A3o%20do%20Perfil.png?raw=true'></center>\n",
    "\n",
    "Para acessarmos as informações dentro do Objeto com o código fonte, usamos o `find()`, passando o elemento e a classe de onde está localizada a seção Introdução (para localizar esses parâmetros basta inspecionar os elementos na página do driver).\n",
    "\n",
    "Uma das complicações do processo de extração está na identificação de onde estão as informações que queremos, pois muitos elementos no código fonte da página mudam conforme a sua seção de login, dessa forma localizar os elementos corretos que não sofrem alterações é um trabalho árduo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6578af63-c8b8-44aa-b53e-ef01de2055b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração da introdução\n",
    "intro = soup.find('div',{'class': 'mt2 relative'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06296007-dc21-45ad-9c1f-8aa017e7dabb",
   "metadata": {},
   "source": [
    "Agora que temos o código localizado da Seção Introdução, podemos extrair as informações: Nome do Candidato, Título Profissional, Localização, Organização Atual e Formação Acadêmica. Como podem haver perfis de candidatos desempregados e também que não possuem formação acadêmica, esses campos precisarão ser condicionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c3e254-02a8-4c44-bbb1-a712acb97a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: Cristina Junqueira \n",
      "Título: Co-Founder at Nubank \n",
      "Localização: São Paulo, São Paulo, Brasil \n",
      "Trabalha em: Nubank \n",
      "Estudou em: Northwestern University - Kellogg School of Management\n"
     ]
    }
   ],
   "source": [
    "# Localizando e extraindo o nome do candidato\n",
    "nome_loc = intro.find('h1')\n",
    "nome = nome_loc.get_text().strip()\n",
    "\n",
    "# Localizando e extraindo o título profissional do candidato\n",
    "titulo_loc = intro.find('div', {'class': 'text-body-medium break-words'})\n",
    "titulo = titulo_loc.get_text().strip()\n",
    "\n",
    "# Localizando e extraindo o endereço de residência\n",
    "localizacao_loc = intro.find('span', {'class': 'text-body-small inline t-black--light break-words'})\n",
    "localizacao = localizacao_loc.get_text().strip()\n",
    "\n",
    "# Localizando e extraindo a organização de trabalho\n",
    "trabalho_loc = intro.find('div', {'class': 'inline-show-more-text'})\n",
    "if trabalho_loc == None:\n",
    "    trabalho = ''\n",
    "else:\n",
    "    trabalho = trabalho_loc.get_text().strip()\n",
    "\n",
    "# Localizando e extraindo a instituição da formação acadêmica\n",
    "estudo_loc = intro.find_all('div', {'class': 'inline-show-more-text'})\n",
    "if estudo_loc == None:\n",
    "    estudo = ''\n",
    "else:\n",
    "    estudo = estudo_loc[1].get_text().strip()\n",
    "\n",
    "# Exibindo os resultados obtidos até agora\n",
    "print('Nome:',nome,\n",
    "      '\\nTítulo:', titulo,\n",
    "      '\\nLocalização:', localizacao,\n",
    "      '\\nTrabalha em:', trabalho,\n",
    "      '\\nEstudou em:', estudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6730d3-1da8-4562-b924-25652995c0e8",
   "metadata": {},
   "source": [
    "## Extraindo a Quantidade de Seguidores da Página\n",
    "\n",
    "Pode ser interessante conhecer o alcance do candidato, se ele é influencer, ou se tem muitas ou poucas conexões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66e9945-af6a-4737-bec8-55e614905b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração da quantidade de seguidores\n",
    "networking = soup.find('ul', {'class':'pv-top-card--list pv-top-card--list-bullet display-flex mt2'}).find('li',{'class':'text-body-small t-black--light inline-block'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(networking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367ed036-3fe8-4500-88de-e2fe75b1a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O perfil possui: 505.607 seguidores\n"
     ]
    }
   ],
   "source": [
    "# Localizando e Extraindo a quantidade de seguidores\n",
    "seguidores_loc = networking.find('span', {'class': 't-bold'})\n",
    "seguidores = seguidores_loc.get_text().strip()\n",
    "\n",
    "# Exibindo a quantidade de seguidores\n",
    "print('O perfil possui:',seguidores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ac380-76ac-47d5-bc98-c3853d88b716",
   "metadata": {},
   "source": [
    "## Extraindo as Informações da Biografia\n",
    "\n",
    "Essa informação estã em uma Seção chamada 'Sobre' no perfil e como também há perfis que não possuem uma Biografia, também teremos que condicionar essa informação. \n",
    "\n",
    "Lembra que comentei sobre os diferentes layouts das páginas dos usuários quando fizemos o Download do Perfil em PDF? Bom, o mesmo problema acontece aqui, em alguns casos o código abaixo funciona corretamente, porém em outros acaba pegando informações erradas, ou seja, é necessário tratar a base de dados gerada após o processo de Web Scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93decf64-5895-4be2-8b8c-f81c5d5e850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração da biografia do candidato\n",
    "sobre_descricao = soup.find('div',{'class':'inline-show-more-text inline-show-more-text--is-collapsed'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(sobre_descricao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce7dbfa-58e2-46e0-a774-86d8df1da28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected to One Year Accelerated ProgramMajors - Finance and Marketing\n"
     ]
    }
   ],
   "source": [
    "# Localizando e extraindo a descrição de perfil na seção Sobre da página\n",
    "if sobre_descricao == None:\n",
    "    sobre = ''\n",
    "else:\n",
    "    sobre_loc = sobre_descricao.find('span', {'aria-hidden': 'true'})\n",
    "    sobre = sobre_loc.get_text().strip()\n",
    "\n",
    "# Visualizando o conteúdo da secção Sobre da página\n",
    "print(sobre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ce4e3-5ab2-489d-afb3-08f93c6e1776",
   "metadata": {},
   "source": [
    "Como podemos ver acima, o perfil em questão infelizmente está pegando uma informação presente na área de formação acadêmica, ou seja, informação no lugar errado, porém com base em outros testes realizados com outros perfis, o código consegue pegar as biografias que realmente estão disponíveis na página, sendo por esse motivo não irei me aprofundar na complexidade de generalizar essa informação para todos os tipos de páginas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa628716-fe89-4fed-8546-d0435a5d7920",
   "metadata": {},
   "source": [
    "## Extraindo as Informações referentes à última Experiência Profissional\n",
    "\n",
    "Essa é uma das seções que começam a complicar bastante a extração do Web Scraping, já que também temos perfis com diferentes layouts nessa seção, por exemplo:\n",
    "\n",
    "<center><img alt=\"Exemplo de Experiência no LinkedIn\" src='https://github.com/octavianosilva/Imagens_projetos/blob/main/exemplo%20de%20experiencia.png?raw=true'></center>\n",
    "\n",
    "Como é possível observar na imagem acima, há três formas de uma pessoa inserir as informações sobre suas experiências, as vezes com informações básicas, outras com descrições das atividades realizadas e também casos como o primeiro item, onde dentro de uma mesma organização a pessoa passou por diversos cargos. Todas essas mudanças de estrutura tornam difícil a generalização do processo de Web Scraping, caindo sempre no mesmo problema que é nem sempre conseguir pegar as informações corretas.\n",
    "\n",
    "A seção Experiência possui uma página exclusiva, o que torna mais fácil extrair as informações do que se fossem extraí-las da página principal, então vamos acessar essa página referente a Experiência Profissional.\n",
    "\n",
    "Como estamos abrindo uma nova página, é importante executar o rolamento dela até o final para pegar todas as informações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "465a931b-f4c3-4638-b52b-901148eacda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Experiência Profissional\n",
    "driver.get('https://www.linkedin.com/in/crisjunqueira/details/experience/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_experiencia = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_experiencia = BeautifulSoup(src_experiencia,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47055202-f9d2-4e69-abad-d8931eef8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração das experiências\n",
    "experiencia = soup_experiencia.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(experiencia[0])\n",
    "\n",
    "# Caso queira outras experiências mais antigas, basta mudar a posição na variável 'experiência[posição]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70a02252-94ff-4a51-959c-c24dd6eae3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para uma determinada estrutura de página\n",
    "def extraindo_info_experiencias(experiencia):\n",
    "    # Obtendo o título da experiência mais recente\n",
    "    titulo_experiencia_loc = experiencia[0].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    titulo_experiencia = titulo_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Separando o nome da organização e o tipo de contrato\n",
    "    organizacao_tipo_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    organizacao_tipo_experiencia = organizacao_tipo_experiencia_loc.get_text().strip()\n",
    "    organizacao_tipo_experiencia = organizacao_tipo_experiencia.split('·')\n",
    "    \n",
    "    # Nome da organização mais recente\n",
    "    organizacao_experiencia = organizacao_tipo_experiencia[0]\n",
    "\n",
    "    # As vezes também temos perfis que não possuem o tipo da contratação especificada no perfil, portanto precisamos de mais um TRY/EXCEPT para contornar esse problema\n",
    "    try:\n",
    "        tipo_contrato_experiencia = organizacao_ultima_experiencia[1].strip()\n",
    "    except:\n",
    "        tipo_contrato_experiencia = ''\n",
    "    \n",
    "    # Separando as datas de início e fim da duração da experiência\n",
    "    tempo_duracao_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    tempo_duracao_experiencia = tempo_duracao_experiencia_loc.get_text().strip().split('·')\n",
    "    tempo_experiencia = tempo_duracao_experiencia[0].split('-')\n",
    "    \n",
    "    # Data de início e fim da experiência\n",
    "    data_inicio_experiencia = tempo_experiencia[0]\n",
    "    data_fim_experiencia = tempo_experiencia[1]\n",
    "    \n",
    "    # Duração da experiência\n",
    "    duracao_experiencia = tempo_duracao_experiencia[1]\n",
    "    \n",
    "    # Retornando as variáveis com as informações extraídas\n",
    "    return titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246348f6-6270-4b84-8fca-991aae917c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para conseguir extrair as informações de outras estruturas de páginas\n",
    "def extraindo_info_experiencias_2(experiencia):\n",
    "    # Obtendo o nome da organização da experiência mais recente\n",
    "    organizacao_experiencia_loc = experiencia[0].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    organizacao_experiencia = organizacao_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Obtendo o tipo de contrato da experiência (nos casos de pessoas que não tenham essa informação, podem aparecer informações erradas)\n",
    "    tipo_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    tipo_experiencia = tipo_experiencia_loc.get_text().strip().split('·')\n",
    "    tipo_experiencia = tipo_experiencia[0].strip()\n",
    "    \n",
    "    # Obtendo o título mais recente da experiência\n",
    "    titulo_experiencia_loc = experiencia[0].find_all('a')\n",
    "    titulo_experiencia_loc = titulo_experiencia_loc[2].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    titulo_experiencia = titulo_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Separando as datas de início e fim da duração da experiência\n",
    "    tempo_duracao_experiencia_loc = experiencia[0].find_all('a')\n",
    "    tempo_duracao_experiencia_loc = tempo_duracao_experiencia_loc[2].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    tempo_duracao_experiencia_loc = tempo_duracao_experiencia_loc.get_text().strip().split('·')\n",
    "    tempo_experiencia = tempo_duracao_experiencia_loc[0].split('-')\n",
    "    \n",
    "    # Datas de início e fim da experiência\n",
    "    data_inicio_experiencia = tempo_experiencia[0]\n",
    "    data_fim_experiencia = tempo_experiencia[1]\n",
    "    \n",
    "    # Duração da experiência\n",
    "    duracao_experiencia = tempo_duracao_experiencia_loc[1]\n",
    "    \n",
    "    # Retorne as variáveis com as informações extraídas\n",
    "    return titulo_experiencia, organizacao_experiencia, tipo_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fd21ce5-644a-4fad-81f6-621f624a80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando as funções\n",
    "try:\n",
    "    try:\n",
    "        titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = extraindo_info_experiencias(experiencia)\n",
    "    except:\n",
    "        titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = extraindo_info_experiencias_2(experiencia)\n",
    "except:\n",
    "    titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = '','','','','',''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ebac376-4b57-48f2-888c-1cb995fc736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título registrado na última experiência profissional: Co-Founder \n",
      "Nome da Organização: Nubank \n",
      "Tipo de Contratação:  \n",
      "Data de Início: mai de 2013  \n",
      "Data de Término:  o momento  \n",
      "Duração da Experiência:  9 anos 7 meses\n"
     ]
    }
   ],
   "source": [
    "# Exibindo o resultado da extração\n",
    "print('Título registrado na última experiência profissional:',titulo_experiencia,\n",
    "      '\\nNome da Organização:', organizacao_experiencia,\n",
    "      '\\nTipo de Contratação:', tipo_contrato_experiencia,\n",
    "      '\\nData de Início:', data_inicio_experiencia,\n",
    "      '\\nData de Término:', data_fim_experiencia,\n",
    "      '\\nDuração da Experiência:', duracao_experiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75001597-b324-4a56-b18d-fb4180b1cc48",
   "metadata": {},
   "source": [
    "## Extraindo as Informações referente à Formação Acadêmica mais Recente\n",
    "\n",
    "<center><img alt=\"Exemplo de Formação Acadêmica no LinkedIn\" src='https://github.com/octavianosilva/Imagens_projetos/blob/main/exemplo%20de%20forma%C3%A7%C3%A3o%20acad%C3%AAmica.png?raw=true'></center>\n",
    "\n",
    "Agora vamos extrair as informações da seção Formação Acadêmica, e aqui também temos mais de um formato de estrutura, ou seja, é necessário criar duas funções para pegar os tipos: informações básicas sobre a formação acadêmica e a estrutura em que o usuário descreve as atividades realizadas durante a formação.\n",
    "\n",
    "Para a Formação Acadêmica, também temos uma página exclusiva, portanto vamos acessá-la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4c6e6e-b0b2-496c-ac07-9d7a2132fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Formação Acadêmica\n",
    "driver.get('https://www.linkedin.com/in/crisjunqueira/details/education/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_educacao = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_educacao = BeautifulSoup(src_educacao,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d74b18-ea20-42d0-9838-b1823226c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração das formações acadêmicas\n",
    "educacao = soup_educacao.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(educacao[0])\n",
    "\n",
    "# Caso queira outras formações mais antigas, basta mudar a posição na variável 'educacao[posição]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a44edd-eda7-4dbe-83ed-3933f66624fc",
   "metadata": {},
   "source": [
    "Como temos pessoas que não possuem estudos ou que não informaram nenhuma Formação Acadêmica, vamos precisar criar uma função para contornar essa situação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1379b42e-6e4a-44e5-924a-d3e65d6b672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para extrair as informações da Formação Acadêmica mais recente\n",
    "def extracao_formacao_academica(educacao):\n",
    "    # Extraindo o nome da Instituição de Ensino\n",
    "    nome_instituicao_loc = educacao[0].find('span', {'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    nome_instituicao = nome_instituicao_loc.get_text().strip()\n",
    "\n",
    "    # Separando o Tipo da Formação do Nome do Formação\n",
    "    tipo_nome_formacao_loc = educacao[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    tipo_nome_formacao = tipo_nome_formacao_loc.get_text().strip().split(',')\n",
    "\n",
    "    # Tipo da Formação\n",
    "    tipo_formacao = tipo_nome_formacao[0].strip()\n",
    "\n",
    "    # Nome da Formação\n",
    "    nome_formacao = tipo_nome_formacao[1].strip()\n",
    "\n",
    "    # Extraindo a duração da Formação\n",
    "    duracao_formacao_loc = educacao[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    duracao_formacao = duracao_formacao_loc.get_text().strip()\n",
    "    \n",
    "    # Retornando as informações\n",
    "    return nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ae8e194-0243-42c9-a5ba-0a16d6ab3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando a função, se não houver Formação Acadêmica retorna as informações nulas\n",
    "try:\n",
    "    nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = extracao_formacao_academica(educacao)\n",
    "except:\n",
    "    nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = '','','',''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2003352c-d816-4ce0-a3c1-0fd3f706aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome da Instituição de Ensino mais recente: Northwestern University - Kellogg School of Management \n",
      "Formação: MBA \n",
      "Título da Formação: Business \n",
      "Duração da Formação: 2007 - 2008\n"
     ]
    }
   ],
   "source": [
    "# Visualizando o resultado da extração\n",
    "print('Nome da Instituição de Ensino mais recente:', nome_instituicao,\n",
    "      '\\nFormação:', tipo_formacao,\n",
    "      '\\nTítulo da Formação:', nome_formacao,\n",
    "      '\\nDuração da Formação:', duracao_formacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2def52-758a-4b4e-a748-ffed487fb416",
   "metadata": {},
   "source": [
    "## Extraindo as Informações referentes as Competências\n",
    "\n",
    "Como essas informações são todas relevantes, iremos armazenar todas as competências em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ba1821-5bdb-4e1a-b9ce-c407b3c23fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Competências\n",
    "driver.get('https://www.linkedin.com/in/crisjunqueira/details/skills/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_competencia = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_competencia = BeautifulSoup(src_competencia,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7827a2f9-7bb0-4105-b73f-d70c5274fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração das competências\n",
    "competencias = soup_competencia.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(competencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "854250db-bc41-4e19-8710-b0d1f152de8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business Strategy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo uma das competências\n",
    "competencias_loc = competencias[0].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "competencia = competencias_loc.get_text().strip()\n",
    "competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49d8ea1a-e7fa-4b21-8d71-91344482a294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business Strategy',\n",
       " 'Strategy',\n",
       " 'Business Planning',\n",
       " 'Strategic Planning',\n",
       " 'Financial Modeling',\n",
       " 'Management Consulting',\n",
       " 'Cross-functional Team Leadership',\n",
       " 'Marketing Strategy',\n",
       " 'Portfolio Management',\n",
       " 'Management',\n",
       " 'Segmentation',\n",
       " 'Product Management',\n",
       " 'Leadership',\n",
       " 'Market Research',\n",
       " 'Business Intelligence',\n",
       " 'Competitive Analysis',\n",
       " 'Business Development',\n",
       " 'Finance',\n",
       " 'Project Management',\n",
       " 'Marketing']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma lista com as competências do perfil\n",
    "lista_competencias = []\n",
    "\n",
    "# Definindo um loop para obter cada competência\n",
    "for p in range(len(competencias)):\n",
    "    competencias_loc = competencias[p].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    competencia_item = competencias_loc.get_text().strip()\n",
    "    lista_competencias.append(competencia_item)\n",
    "\n",
    "# Exibindo o resultado da extração de todas as competências\n",
    "lista_competencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f9d5e-e36f-4ebd-9e8d-b97b3e63ded0",
   "metadata": {},
   "source": [
    "## Extraindo as Informações sobre os Idiomas\n",
    "\n",
    "Também é interessante conhecer se o candidato possui proficiência em outras línguas, portanto vamos aproveitar que a Seção Idiomas também possui uma página exclusiva e extrair essas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2ea097-9377-4317-9abc-0b378d0d3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Idiomas\n",
    "driver.get('https://www.linkedin.com/in/crisjunqueira/details/languages/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_idiomas = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_idiomas = BeautifulSoup(src_idiomas,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641ae196-8935-4d26-9730-7e96733a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração dos idiomas\n",
    "idiomas = soup_idiomas.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(idiomas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b076d135-aa3e-49e5-b536-2a02a996dcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English (Nível avançado)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraindo o primeiro idioma e acrescentando sua proficiência em '()'\n",
    "idiomas_loc = idiomas[0].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "idiomas_item = idiomas_loc.get_text().strip()\n",
    "\n",
    "proficiencia_loc = idiomas[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "proficiencia_item = proficiencia_loc.get_text().strip()\n",
    "\n",
    "# Unindo as duas informações em uma única String\n",
    "idioma_proficiencia = '{} ({})'.format(idiomas_item,proficiencia_item)\n",
    "\n",
    "# Visualizando o resultado\n",
    "idioma_proficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e67534-97c4-4019-9242-a29b1e59c968",
   "metadata": {},
   "source": [
    "Agora vamos criar um loop para extrair todas as línguas descritas no perfil, mas como também temos perfis que não possuem uma seção de Idiomas, vamos criar uma função para contornar essa situação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4536dfbf-4de7-4034-bbe2-6f899c3e87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para extrair as informações sobre as línguas do candidato\n",
    "def extraindo_idiomas(idiomas):\n",
    "    # Criando uma lista com as línguas do perfil\n",
    "    lista_linguas = []\n",
    "\n",
    "    # Definindo um loop para obter cada língua\n",
    "    for p in range(len(idiomas)):\n",
    "        # Extraindo o nome da língua\n",
    "        idiomas_loc = idiomas[p].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "        idiomas_item = idiomas_loc.get_text().strip()\n",
    "\n",
    "        # Extraindo o nível de proficiência\n",
    "        try:\n",
    "            proficiencia_loc = idiomas[p].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "            proficiencia_item = proficiencia_loc.get_text().strip()\n",
    "\n",
    "            # Unindo as duas informações em uma única String\n",
    "            idioma_proficiencia = '{} ({})'.format(idiomas_item,proficiencia_item)\n",
    "\n",
    "        except: # Caso a pessoa não informe o nível de proficiência\n",
    "            idioma_proficiencia = idiomas_item\n",
    "\n",
    "        lista_linguas.append(idioma_proficiencia)\n",
    "        \n",
    "    # Retornando a lista com os idiomas\n",
    "    return lista_linguas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0feb38f2-f283-4891-bf88-9be03f78295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando a função para extrair as informações referentes aos idiomas para caso estejam disponíveis\n",
    "try:\n",
    "    lista_linguas = extraindo_idiomas(idiomas)\n",
    "except:\n",
    "    lista_linguas = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1ab249a-cbf6-457c-9e1a-5b2ab435a5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English (Nível avançado)',\n",
       " 'Portuguese (Fluente ou nativo)',\n",
       " 'Spanish (Nível básico)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibindo o resultado\n",
    "lista_linguas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53fc4d8-b36e-434b-9e5f-c405d8f013e6",
   "metadata": {},
   "source": [
    "## Extraindo as Informações referentes aos Prêmios e Honrarias do Candidato\n",
    "\n",
    "Há pessoas que participam de competições ou recebem premiações como reconhecimento por seus trabalhos que disponibilizam essas conquistas em seus perfis, o que é uma informação valiosa para um recrutador.\n",
    "\n",
    "Essa Seção de Honrarias também possui uma página exclusiva, portanto vamos acessá-la. Para esse caso iremos utilizar outro perfil já que o atual não possui informações sobre Honrarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59381d5-030c-49e7-aac0-28aaee9db48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Honrarias\n",
    "driver.get('https://www.linkedin.com/in/anitta-larissa-machado-a1118818a/details/honors/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_honras = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_honras = BeautifulSoup(src_honras,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68ff5457-26be-4cc6-9efb-10627950b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o HTML de extração das honrarias\n",
    "honras = soup_honras.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Visualizando o código HTML\n",
    "#print(honras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4778863-eee9-48a6-8707-3f4545c7c952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prêmio ASCAP Latin Music Awards (jan de 2019)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraindo a primeira honraria e acrescentando a data em '()'\n",
    "honra_loc = honras[0].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "honra_item = honra_loc.get_text().strip()\n",
    "\n",
    "# Extraindo a data da honraria\n",
    "try:\n",
    "    data_loc = honras[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    data_item = data_loc.get_text().strip()\n",
    "\n",
    "    # Unindo as duas informações em uma única String\n",
    "    honra_data = '{} ({})'.format(honra_item,data_item)\n",
    "\n",
    "except: # Caso a pessoa não informe a data da honraria\n",
    "    honra_data = honra_item\n",
    "\n",
    "# Visualizando o resultado\n",
    "honra_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e5a1f-059d-43dd-bf20-a93996b30524",
   "metadata": {},
   "source": [
    "Como temos pessoas que não possuem essa informação disponível, temos que criar uma função para contornar a situação e também para retornar uma lista com todas as honrarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c431b606-0011-412e-be20-7c428591652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para extrair as informações sobre as honrarias do candidato\n",
    "def extraindo_honrarias(honras):\n",
    "    # Criando uma lista para as honrarias do perfil\n",
    "    lista_honras = []\n",
    "\n",
    "    # Definindo um loop para obter cada honraria\n",
    "    for p in range(len(honras)):\n",
    "        # Extraindo a primeira honraria e acrescentando a data em '()'\n",
    "        honra_loc = honras[p].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "        honra_item = honra_loc.get_text().strip()\n",
    "\n",
    "        # Extraindo a data da honraria\n",
    "        try:\n",
    "            data_loc = honras[p].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "            data_item = data_loc.get_text().strip()\n",
    "\n",
    "            # Unindo as duas informações em uma única String\n",
    "            honra_data = '{} ({})'.format(honra_item,data_item)\n",
    "\n",
    "        except: # Caso a pessoa não informe a data da honraria\n",
    "            honra_data = honra_item\n",
    "\n",
    "        lista_honras.append(honra_data)\n",
    "        \n",
    "    # Retornando a lista com as honrarias\n",
    "    return lista_honras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04fe9ea3-df66-4a83-85d2-6860e1f9ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando a função\n",
    "try:\n",
    "    lista_honras = extraindo_honrarias(honras)\n",
    "except: # Caso o perfil não possua honrarias\n",
    "    lista_honras = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a91edaf5-0a05-48ca-b4f7-d908789fccbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prêmio ASCAP Latin Music Awards (jan de 2019)',\n",
       " 'Prêmio Multishow de Música Brasileira (jan de 2019)',\n",
       " 'Troféu Imprensa SBT (jan de 2019)',\n",
       " 'Jingle Bells Awards (jan de 2018)',\n",
       " 'Latin American Music Awards (jan de 2018)',\n",
       " 'Latin Music Italian Awards de Milão/Itália (jan de 2018)',\n",
       " 'MTV Millennial Awards México (jan de 2018)',\n",
       " 'Melhores do Ano da Rede Globo (jan de 2018)',\n",
       " 'Meus Prêmios Nick (jan de 2018)',\n",
       " 'Prêmio \"Melhor Clipe do Ano\" do MTV Israel: Desfile Anual (jan de 2018)',\n",
       " 'Prêmio F5 da Folha de São Paulo (jan de 2018)',\n",
       " 'Prêmio Faz Diferença (jan de 2018)',\n",
       " 'Prêmio MTV Europe Music Award (jan de 2018)',\n",
       " 'Prêmio MTV Millennial Awards Brasil (jan de 2018)',\n",
       " 'Prêmio TeleHit (jan de 2018)',\n",
       " 'Prêmio da Revista Mexicana \"Eres\" (jan de 2018)',\n",
       " 'iHeartRadio Music Awards (jan de 2018)',\n",
       " 'Capricho Awards (jan de 2017)',\n",
       " 'Melhores do Ano FM O Dia (jan de 2017)',\n",
       " 'Prêmio CONTIGO! Online (jan de 2017)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando o resultado\n",
    "lista_honras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a427ae-30c4-49ff-b27a-c6fc1274f815",
   "metadata": {},
   "source": [
    "## Extraindo a Quantidade de Recomendações o Candidato possui\n",
    "\n",
    "Também temos aqueles perfis que possuem recomendações de outros profissionais, o que é outra informação valiosa para o recrutador.\n",
    "\n",
    "Como também temos uma página exclusiva para essa Seção de Recomendações, fica mais fácil extrair essas informações.\n",
    "\n",
    "Aqui também usaremos outro perfil que possui recomendações, para demonstrar o funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e4bb17d-ffaa-4a48-a674-f080e9acb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a página de Recomendações\n",
    "driver.get('https://www.linkedin.com/in/shilpar/details/recommendations/')\n",
    "\n",
    "# Lendo a página do início ao fim\n",
    "inicio = time.time()\n",
    "posicao_inicial_rolamento = 0\n",
    "posicao_final_rolamento = 1000\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "   \n",
    "    posicao_inicial_rolamento = posicao_final_rolamento\n",
    "    posicao_final_rolamento += 1000\n",
    "    \n",
    "    # Aguardando 3 segundos\n",
    "    time.sleep(3)\n",
    "    \n",
    "    fim = time.time()\n",
    "    \n",
    "    # Executar o script por 20 segundos\n",
    "    if round(fim - inicio) > 20:\n",
    "        break\n",
    "\n",
    "# Salvando o código fonte da página em uma variável\n",
    "src_recomendacoes = driver.page_source\n",
    "\n",
    "# Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "soup_recomendacoes = BeautifulSoup(src_recomendacoes,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd4a1efb-b81a-48af-9877-b26bb1d171ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Obtendo o HTML de extração das recomendações\n",
    "recomendacoes = soup_recomendacoes.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "# Obtendo a quantidade de Recomendações do Perfil\n",
    "print(len(recomendacoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d606b9-6def-48bd-9015-5d4dcd792d62",
   "metadata": {},
   "source": [
    "Como nem todos os perfis possuem recomendações precisamos criar uma condição, além disso mesmo que o perfil não possua nenhuma recomendação, ele receberá o valor 1, ou seja, estaria sendo computada uma recomendação de forma errada.\n",
    "\n",
    "Isso se dá pelo fato de que mesmo sem nenhuma recomendação, o código HTML possui ao menos um elemento 'li', que quando chamandos `len()` retorna o valor 1. Para contornar esse problema iremos aprofundar no código HTML apenas para validar se há uma recomendação ou não.\n",
    "\n",
    "Resumindo, se não houver recomendações, o código tentará acessar um elemento que não existe, causando erro e saindo do `TRY` para o `EXCEPT`, que então retornará o valor 0 corretamente. Caso haja uma recomendação, o elemento será encontrado e o `TRY` irá prosseguir normalmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fcbdade-5c1f-42e6-ae1d-7cd3ea78f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos acessar a primeira recomendação somente para validar\n",
    "validando_recomendacoes = recomendacoes[0].find('span',{'class':'mr1 hoverable-link-text t-bold'})\n",
    "\n",
    "# Caso a validação retorne None, recomendações = 0\n",
    "if validando_recomendacoes == None:\n",
    "\n",
    "    # Obtendo a quantidade de recomendações do perfil\n",
    "    qtde_recomendacoes = 0 \n",
    "    \n",
    "else: # Caso haja recomendações\n",
    "    \n",
    "    # Obtendo a quantidade de recomendações do perfil\n",
    "    qtde_recomendacoes = len(recomendacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79a9d781-6e41-40a1-86e3-98067da88f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibindo o resultado\n",
    "qtde_recomendacoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28819d10-2f03-42cb-8838-6a1b33c43e79",
   "metadata": {},
   "source": [
    "## Definindo uma Pipeline para Extrair as Informações de vários Candidatos\n",
    "\n",
    "Agora que vimos que é possível extrair as informações para uma única pessoa, vamos definir uma pipeline para acessar cada URL dos candidatos que aplicaram nas vagas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472f772-7821-488c-b5ea-4871945de8fe",
   "metadata": {},
   "source": [
    "### Modulos (área com todas as funções utilizadas na pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d1c1339-9f60-497b-b014-eafea3c2d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Logar na Plataforma LinkedIn\n",
    "def logar_linkedin(login, password):\n",
    "    # Abrindo a página de Login do LinkedIn\n",
    "    driver.get('https://www.linkedin.com/login/')\n",
    "    time.sleep(2) # aguardar 2 segundos para a página abrir\n",
    "\n",
    "    # Inserindo o usuário\n",
    "    usuario = driver.find_element('id','username') # acessando o campo usuário\n",
    "    usuario.send_keys(login) # enviando a string com o usuário\n",
    "    \n",
    "    # Inserindo a senha\n",
    "    senha = driver.find_element('id','password') # acessando o campo senha\n",
    "    senha.send_keys(password) # enviando a string com a senha\n",
    "    \n",
    "    # Clicando no botão 'Entrar' para acessar a conta no LinkedIn\n",
    "    driver.find_element(\"xpath\",\"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0b6dcc8-d43e-44da-885c-17792b0ba081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para baixar o Perfil em formato PDF (estilo Currículo)\n",
    "def baixar_perfil_pdf():\n",
    "    try:\n",
    "        try:\n",
    "            # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "            driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/button').click()                               \n",
    "            time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "            # Clicando no botão \"Salvar como PDF\"\n",
    "            driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/div/div/ul/li[3]/div').click()\n",
    "        \n",
    "        except:\n",
    "            # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "            driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/button').click()\n",
    "            time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "            # Clicando no botão \"Salvar como PDF\"\n",
    "            driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/div/div/ul/li[3]/div').click()\n",
    "            \n",
    "    except:\n",
    "        try:\n",
    "            try:\n",
    "                # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "                driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/button').click()\n",
    "                time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "                # Clicando no botão \"Salvar como PDF\"\n",
    "                driver.find_element('xpath','/html/body/div[5]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/div/div/ul/li[3]/div').click()\n",
    "            \n",
    "            except:\n",
    "                # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "                driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/button').click()           \n",
    "                time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "                # Clicando no botão \"Salvar como PDF\"\n",
    "                driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[3]/div/div/ul/li[3]/div').click()\n",
    "\n",
    "        except:\n",
    "            # Clicando no botão '...' ou 'Mais' na seção de \"Introdução\" do perfil\n",
    "            driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/button').click()\n",
    "            time.sleep(1) # aguardar 1 segundo\n",
    "\n",
    "            # Clicando no botão \"Salvar como PDF\"\n",
    "            driver.find_element('xpath','/html/body/div[4]/div[3]/div/div/div[2]/div/div/main/section[1]/div[2]/div[3]/div/div[2]/div/div/ul/li[3]/div').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a125a51d-f1dc-4b52-80c4-074d8ef05f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler a página inicial\n",
    "def ler_pagina_principal():\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 2 segundos\n",
    "        time.sleep(2)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 10 segundos\n",
    "        if round(fim - inicio) > 10:\n",
    "            break\n",
    "    \n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    \n",
    "    # Retornando o Código Fonte\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02687c45-c537-4c81-a6c3-9031c356179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Extrair as informações contidas na seção de Introdução da página\n",
    "def extrair_introducao(codigo_fonte_principal):\n",
    "    # Obtendo o HTML de extração da introdução\n",
    "    intro = codigo_fonte_principal.find('div',{'class': 'mt2 relative'})\n",
    "    \n",
    "    # Localizando e extraindo o nome do candidato\n",
    "    nome_loc = intro.find('h1')\n",
    "    nome = nome_loc.get_text().strip()\n",
    "\n",
    "    # Localizando e extraindo o título profissional do candidato\n",
    "    titulo_loc = intro.find('div', {'class': 'text-body-medium break-words'})\n",
    "    titulo = titulo_loc.get_text().strip()\n",
    "\n",
    "    # Localizando e extraindo o endereço de residência\n",
    "    try:\n",
    "        localizacao_loc = intro.find('span', {'class': 'text-body-small inline t-black--light break-words'})\n",
    "        localizacao = localizacao_loc.get_text().strip()\n",
    "    except:\n",
    "        localizacao = ''\n",
    "\n",
    "    # Localizando e extraindo a organização de trabalho\n",
    "    trabalho_loc = intro.find('div', {'class': 'inline-show-more-text'})\n",
    "    if trabalho_loc == None:\n",
    "        trabalho = ''\n",
    "    else:\n",
    "        trabalho = trabalho_loc.get_text().strip()\n",
    "\n",
    "    # Localizando e extraindo a instituição da formação acadêmica\n",
    "    estudo_loc = intro.find_all('div', {'class': 'inline-show-more-text'})\n",
    "    if estudo_loc == None:\n",
    "        estudo = ''\n",
    "    else:\n",
    "        try:\n",
    "            estudo = estudo_loc[1].get_text().strip()\n",
    "        except:\n",
    "            estudo = ''\n",
    "        \n",
    "    # Retornando as informações extraídas\n",
    "    return nome, titulo, localizacao, trabalho, estudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9d5d06e-3843-4d5e-a565-5be3b8b8b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a quantidade de Seguidores da página\n",
    "def extrair_seguidores(codigo_fonte_principal):\n",
    "    # Obtendo o HTML de extração da quantidade de seguidores\n",
    "    try:\n",
    "        networking = codigo_fonte_principal.find('ul', {'class':'pv-top-card--list pv-top-card--list-bullet display-flex mt2'}).find('li',{'class':'text-body-small t-black--light inline-block'})\n",
    "\n",
    "        # Localizando e extraindo a quantidade de seguidores\n",
    "        seguidores_loc = networking.find('span', {'class': 't-bold'})\n",
    "        seguidores = seguidores_loc.get_text().strip()\n",
    "        \n",
    "    except:\n",
    "        networking = codigo_fonte_principal.find('p', {'class':'pvs-header__subtitle text-body-small'})\n",
    "\n",
    "        # Localizando e extraindo a quantidade de seguidores\n",
    "        seguidores_loc = networking.find('span', {'aria-hidden': 'true'})\n",
    "        seguidores = seguidores_loc.get_text().strip()\n",
    "    \n",
    "    # Retornando a informação extraída\n",
    "    return seguidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2250620f-01fd-4e09-bee1-f67bf39e7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a Biografia da página\n",
    "def extrair_biografia(codigo_fonte_principal):\n",
    "    # Obtendo o HTML de extração da biografia do candidato\n",
    "    sobre_descricao = codigo_fonte_principal.find('div',{'class':'inline-show-more-text inline-show-more-text--is-collapsed'})\n",
    "    \n",
    "    # Localizando e extraindo a descrição de perfil na seção Sobre da página\n",
    "    if sobre_descricao == None:\n",
    "        sobre = ''\n",
    "    else:\n",
    "        sobre_loc = sobre_descricao.find('span', {'aria-hidden': 'true'})\n",
    "        sobre = sobre_loc.get_text().strip()\n",
    "        \n",
    "    # Retornando as informações extraídas\n",
    "    return sobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a979e572-7890-4a91-8198-24a8497a9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a experiência profissional TIPO 1\n",
    "def extrair_info_experiencias(experiencia):\n",
    "    # Obtendo o título da experiência mais recente\n",
    "    titulo_experiencia_loc = experiencia[0].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    titulo_experiencia = titulo_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Separando o nome da organização e o tipo de contrato\n",
    "    organizacao_tipo_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    organizacao_tipo_experiencia = organizacao_tipo_experiencia_loc.get_text().strip()\n",
    "    organizacao_tipo_experiencia = organizacao_tipo_experiencia.split('·')\n",
    "    \n",
    "    # Nome da organização mais recente\n",
    "    organizacao_experiencia = organizacao_tipo_experiencia[0]\n",
    "\n",
    "    # As vezes também temos perfis que não possuem o tipo da contratação especificada no perfil, portanto precisamos de mais um TRY/EXCEPT para contornar esse problema\n",
    "    try:\n",
    "        tipo_contrato_experiencia = organizacao_tipo_experiencia[1].strip()\n",
    "    except:\n",
    "        tipo_contrato_experiencia = ''\n",
    "    \n",
    "    # Separando as datas de início e fim da duração da experiência\n",
    "    tempo_duracao_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    tempo_duracao_experiencia = tempo_duracao_experiencia_loc.get_text().strip().split('·')\n",
    "    tempo_experiencia = tempo_duracao_experiencia[0].split('-')\n",
    "    \n",
    "    # Data de início e fim da experiência\n",
    "    data_inicio_experiencia = tempo_experiencia[0].strip()\n",
    "    data_fim_experiencia = tempo_experiencia[1].strip()\n",
    "    \n",
    "    # Duração da experiência\n",
    "    duracao_experiencia = tempo_duracao_experiencia[1].strip()\n",
    "    \n",
    "    # Retornando as variáveis com as informações extraídas\n",
    "    return titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66150415-9199-4827-b44c-e1265f8fec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a experiência profissional TIPO 2\n",
    "def extrair_info_experiencias_2(experiencia):\n",
    "    # Obtendo o nome da organização da experiência mais recente\n",
    "    organizacao_experiencia_loc = experiencia[0].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    organizacao_experiencia = organizacao_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Obtendo o tipo de contrato da experiência (nos casos de pessoas que não tenham essa informação, podem aparecer informações erradas)\n",
    "    tipo_experiencia_loc = experiencia[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    tipo_experiencia = tipo_experiencia_loc.get_text().strip().split('·')\n",
    "    tipo_experiencia = tipo_experiencia[0].strip()\n",
    "    \n",
    "    # Obtendo o título mais recente da experiência\n",
    "    titulo_experiencia_loc = experiencia[0].find_all('a')\n",
    "    titulo_experiencia_loc = titulo_experiencia_loc[2].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    titulo_experiencia = titulo_experiencia_loc.get_text().strip()\n",
    "    \n",
    "    # Separando as datas de início e fim da duração da experiência\n",
    "    tempo_duracao_experiencia_loc = experiencia[0].find_all('a')\n",
    "    tempo_duracao_experiencia_loc = tempo_duracao_experiencia_loc[2].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    tempo_duracao_experiencia_loc = tempo_duracao_experiencia_loc.get_text().strip().split('·')\n",
    "    tempo_experiencia = tempo_duracao_experiencia_loc[0].split('-')\n",
    "    \n",
    "    # Datas de início e fim da experiência\n",
    "    data_inicio_experiencia = tempo_experiencia[0].strip()\n",
    "    data_fim_experiencia = tempo_experiencia[1].strip()\n",
    "    \n",
    "    # Duração da experiência\n",
    "    duracao_experiencia = tempo_duracao_experiencia_loc[1]\n",
    "    \n",
    "    # Retorne as variáveis com as informações extraídas\n",
    "    return titulo_experiencia, organizacao_experiencia, tipo_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5b065b5-8e69-4969-8f0f-a5d0688e297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as informações referentes à última experiência profissional\n",
    "def extrair_experiencia_profissional(perfil_id):\n",
    "    # Acessando a página referente às experiências profissionais\n",
    "    experiencia_url = 'https://www.linkedin.com/in/{}/details/experience/'.format(perfil_id)\n",
    "    driver.get(experiencia_url)\n",
    "\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 2 segundos\n",
    "        time.sleep(2)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 5 segundos\n",
    "        if round(fim - inicio) > 5:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_experiencia = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_experiencia = BeautifulSoup(src_experiencia,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração das experiências\n",
    "    experiencia = soup_experiencia.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "    # Executando as funções\n",
    "    try:\n",
    "        try:\n",
    "            titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = extrair_info_experiencias(experiencia)\n",
    "        except:\n",
    "            titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = extrair_info_experiencias_2(experiencia)\n",
    "    except:\n",
    "        titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = ' ',' ',' ',' ',' ',' '\n",
    "        \n",
    "    # Retornando as informações extraídas\n",
    "    return titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "843b3647-3428-4298-8bf7-c231c230429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as informações da Formação Acadêmica mais recente\n",
    "def extrair_info_formacao_academica(educacao):\n",
    "    # Extraindo o nome da Instituição de Ensino\n",
    "    nome_instituicao_loc = educacao[0].find('span', {'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    nome_instituicao = nome_instituicao_loc.get_text().strip()\n",
    "\n",
    "    # Separando o Tipo da Formação do Nome do Formação\n",
    "    tipo_nome_formacao_loc = educacao[0].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "    tipo_nome_formacao = tipo_nome_formacao_loc.get_text().strip().split(',')\n",
    "\n",
    "    # Tipo da Formação\n",
    "    tipo_formacao = tipo_nome_formacao[0].strip()\n",
    "\n",
    "    # Nome da Formação\n",
    "    nome_formacao = tipo_nome_formacao[1].strip()\n",
    "\n",
    "    # Extraindo a duração da Formação\n",
    "    duracao_formacao_loc = educacao[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "    duracao_formacao = duracao_formacao_loc.get_text().strip()\n",
    "    \n",
    "    # Retornando as informações\n",
    "    return nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4dc32f8-1501-4814-ba6a-7409bf8c42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as informações da Formação Acadêmica mais recente TIPO 2 (onde não tem o nome da formação ou tipo dela)\n",
    "def extrair_info_formacao_academica_2(educacao):\n",
    "    # Extraindo o nome da Instituição de Ensino\n",
    "    nome_instituicao_loc = educacao[0].find('span', {'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "    nome_instituicao = nome_instituicao_loc.get_text().strip()\n",
    "\n",
    "    # Extraindo a duração da Formação (caso esteja disponível)\n",
    "    try:\n",
    "        duracao_formacao_loc = educacao[0].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "        duracao_formacao = duracao_formacao_loc.get_text().strip()\n",
    "    except:\n",
    "        duracao_formacao = ''\n",
    "    \n",
    "    # Campos não disponíveis `nome_formacao` e `tipo_formacao`\n",
    "    nome_formacao = ''\n",
    "    tipo_formacao = ''\n",
    "    \n",
    "    # Retornando as informações\n",
    "    return nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d787067-52ed-43de-983e-08cee0b65ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as informações referente à Formação Acadêmica mais recente\n",
    "def extrair_formacao_academica(perfil_id):\n",
    "    # Acessando a página referente às formações acadêmicas\n",
    "    formacao_url = 'https://www.linkedin.com/in/{}/details/education/'.format(perfil_id)\n",
    "    driver.get(formacao_url)\n",
    "    \n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 2 segundos\n",
    "        time.sleep(2)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 5 segundos\n",
    "        if round(fim - inicio) > 5:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_educacao = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_educacao = BeautifulSoup(src_educacao,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração das formações acadêmicas\n",
    "    educacao = soup_educacao.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "    # Executando a função, se não houver Formação Acadêmica retorna as informações nulas\n",
    "    try:\n",
    "        try:\n",
    "            nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = extrair_info_formacao_academica(educacao)\n",
    "        except:\n",
    "            nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = extrair_info_formacao_academica_2(educacao)\n",
    "    except:\n",
    "        nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = '','','',''\n",
    "        \n",
    "    # Retornando as informações extraídas\n",
    "    return nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "971a019f-ebca-46e1-bf86-5e7cf46be776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a lista de Competências do Candidato\n",
    "def extrair_competencias(perfil_id):\n",
    "    # Acessando a página de Competências\n",
    "    competencias_url = 'https://www.linkedin.com/in/{}/details/skills/'.format(perfil_id)\n",
    "    driver.get(competencias_url)\n",
    "\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 2 segundos\n",
    "        time.sleep(2)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 10 segundos\n",
    "        if round(fim - inicio) > 10:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_competencia = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_competencia = BeautifulSoup(src_competencia,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração das competências\n",
    "    competencias = soup_competencia.find('ul',{'class':'pvs-list'})\n",
    "\n",
    "    # Criando uma lista com as competências do perfil\n",
    "    lista_competencias = []\n",
    "\n",
    "    # Caso a pessoa não tenha nenhuma competência disponível, o código HTML retornará 'None' \n",
    "    if competencias == None:\n",
    "        lista_competencias = ''\n",
    "    \n",
    "    else: # Se o código HTML não for 'None', acessar o HTML com as lista das competências\n",
    "        # Acessando o HTML das listas das competências\n",
    "        competencias = soup_competencia.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "        \n",
    "        # Definindo um loop para extrair cada competência disponível\n",
    "        for p in range(len(competencias)):\n",
    "            competencias_loc = competencias[p].find('span',{'class':'mr1 hoverable-link-text t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "            competencia_item = competencias_loc.get_text().strip()\n",
    "            lista_competencias.append(competencia_item)\n",
    "\n",
    "    # Retornando a lista das competências\n",
    "    return lista_competencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0fbce4e-8353-4958-aaae-5b9058e5bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para contornar o problema de páginas que não possuem Idiomas\n",
    "def extrair_info_idiomas(idiomas):\n",
    "    # Criando uma lista com as línguas do perfil\n",
    "    lista_linguas = []\n",
    "\n",
    "    # Definindo um loop para obter cada língua\n",
    "    for p in range(len(idiomas)):\n",
    "        # Extraindo o nome da língua\n",
    "        idiomas_loc = idiomas[p].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "        idiomas_item = idiomas_loc.get_text().strip()\n",
    "\n",
    "        # Extraindo o nível de proficiência\n",
    "        try:\n",
    "            proficiencia_loc = idiomas[p].find('span',{'class':'t-14 t-normal t-black--light'}).find('span',{'aria-hidden':'true'})\n",
    "            proficiencia_item = proficiencia_loc.get_text().strip()\n",
    "\n",
    "            # Unindo as duas informações em uma única String\n",
    "            idioma_proficiencia = '{} ({})'.format(idiomas_item,proficiencia_item)\n",
    "\n",
    "        except: # Caso a pessoa não informe o nível de proficiência\n",
    "            idioma_proficiencia = idiomas_item\n",
    "\n",
    "        lista_linguas.append(idioma_proficiencia)\n",
    "        \n",
    "    # Retornando a lista com os idiomas\n",
    "    return lista_linguas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0055c4eb-484a-459d-9f6c-3b62f1eec200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as informações referentes aos Idiomas dominados pelo candidato\n",
    "def extrair_idiomas(perfil_id):\n",
    "    # Acessando a página de Idiomas\n",
    "    idiomas_url = 'https://www.linkedin.com/in/{}/details/languages/'.format(perfil_id)\n",
    "    driver.get(idiomas_url)\n",
    "\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 2 segundos\n",
    "        time.sleep(2)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 5 segundos\n",
    "        if round(fim - inicio) > 5:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_idiomas = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_idiomas = BeautifulSoup(src_idiomas,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração dos idiomas\n",
    "    idiomas = soup_idiomas.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "    \n",
    "    # Executando a função para extrair as informações referentes aos idiomas para caso estejam disponíveis\n",
    "    try:\n",
    "        lista_linguas = extrair_info_idiomas(idiomas)\n",
    "    except:\n",
    "        lista_linguas = ''\n",
    "        \n",
    "    # Retornando a lista com os idiomas do candidato\n",
    "    return lista_linguas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b3d7688-d158-4b4a-965c-a2947f66feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para contornar o problema com as páginas que não possuem honrarias\n",
    "def extrair_info_honrarias(honras):\n",
    "    # Criando uma lista para as honrarias do perfil\n",
    "    lista_honras = []\n",
    "\n",
    "    # Definindo um loop para obter cada honraria\n",
    "    for p in range(len(honras)):\n",
    "        # Extraindo a primeira honraria e acrescentando a data em '()'\n",
    "        honra_loc = honras[p].find('span',{'class':'mr1 t-bold'}).find('span',{'aria-hidden':'true'})\n",
    "        honra_item = honra_loc.get_text().strip()\n",
    "\n",
    "        # Extraindo a data da honraria\n",
    "        try:\n",
    "            data_loc = honras[p].find('span',{'class':'t-14 t-normal'}).find('span',{'aria-hidden':'true'})\n",
    "            data_item = data_loc.get_text().strip()\n",
    "\n",
    "            # Unindo as duas informações em uma única String\n",
    "            honra_data = '{} ({})'.format(honra_item,data_item)\n",
    "\n",
    "        except: # Caso a pessoa não informe a data da honraria\n",
    "            honra_data = honra_item\n",
    "\n",
    "        lista_honras.append(honra_data)\n",
    "        \n",
    "    # Retornando a lista com as honrarias\n",
    "    return lista_honras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bebe0820-097d-4063-aea9-d4f71374335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair as Honrarias e Premiações\n",
    "def extrair_honrarias(perfil_id):\n",
    "    # Acessando a página de Honrarias\n",
    "    honrarias_url = 'https://www.linkedin.com/in/{}/details/honors/'.format(perfil_id)\n",
    "    driver.get(honrarias_url)\n",
    "\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 3 segundos\n",
    "        time.sleep(3)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 5 segundos\n",
    "        if round(fim - inicio) > 5:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_honras = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_honras = BeautifulSoup(src_honras,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração das honrarias\n",
    "    honras = soup_honras.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "    \n",
    "    # Executando a função\n",
    "    try:\n",
    "        lista_honras = extrair_info_honrarias(honras)\n",
    "    except: # Caso o perfil não possua honrarias\n",
    "        lista_honras = ''\n",
    "        \n",
    "    # Retornando a lista com as honrarias\n",
    "    return lista_honras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07feec96-33a1-4aa0-9871-0e60d3a713cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair a quantidade de Recomendações que o candidato possui\n",
    "def extrair_recomendacoes(perfil_id):\n",
    "    # Acessando a página de Recomendações\n",
    "    recomendacoes_url = 'https://www.linkedin.com/in/{}/details/recommendations/'.format(perfil_id)\n",
    "    driver.get(recomendacoes_url)\n",
    "\n",
    "    # Lendo a página do início ao fim\n",
    "    inicio = time.time()\n",
    "    posicao_inicial_rolamento = 0\n",
    "    posicao_final_rolamento = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({posicao_inicial_rolamento},{posicao_final_rolamento})\")\n",
    "\n",
    "        posicao_inicial_rolamento = posicao_final_rolamento\n",
    "        posicao_final_rolamento += 1000\n",
    "\n",
    "        # Aguardando 3 segundos\n",
    "        time.sleep(3)\n",
    "\n",
    "        fim = time.time()\n",
    "\n",
    "        # Executar o script por 5 segundos\n",
    "        if round(fim - inicio) > 5:\n",
    "            break\n",
    "\n",
    "    # Salvando o código fonte da página em uma variável\n",
    "    src_recomendacoes = driver.page_source\n",
    "\n",
    "    # Utilizando o código fonte para gerar um objeto Beautiful Soup\n",
    "    soup_recomendacoes = BeautifulSoup(src_recomendacoes,'lxml')\n",
    "    \n",
    "    # Obtendo o HTML de extração das recomendações\n",
    "    recomendacoes = soup_recomendacoes.find('ul',{'class':'pvs-list'}).find_all('li',{'class':'pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated'})\n",
    "\n",
    "    # Vamos acessar a primeira recomendação somente para validar\n",
    "    validando_recomendacoes = recomendacoes[0].find('span',{'class':'mr1 hoverable-link-text t-bold'})\n",
    "\n",
    "    # Caso a validação retorne None, recomendações = 0\n",
    "    if validando_recomendacoes == None:\n",
    "\n",
    "        # Obtendo a quantidade de recomendações do perfil\n",
    "        qtde_recomendacoes = 0 \n",
    "    \n",
    "    else: # Caso haja recomendações\n",
    "    \n",
    "        # Obtendo a quantidade de recomendações do perfil\n",
    "        qtde_recomendacoes = len(recomendacoes)\n",
    "          \n",
    "    # Retornando a informação extraída\n",
    "    return qtde_recomendacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d7106c8-54f5-4340-80e2-43d43c843564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de Execução da Pipeline\n",
    "def executar_webscraping(login, password, lista_candidatos):\n",
    "    \n",
    "    # Logando no LinkedIn\n",
    "    logar_linkedin(login, password)\n",
    "    \n",
    "    # Estruturando o DataFrame para receber as informações da extração\n",
    "    df = pd.DataFrame(columns=['Nome Candidato', 'Título do Candidato', 'Localização', 'Organização Atual', 'Estudou em', 'Quantidade de Seguidores',\n",
    "                               'Biografia', 'Última Experiência', 'Tipo da Experiência', 'Cargo da Experiência', 'Início da Experiência', 'Fim da Experiência',\n",
    "                               'Duração da Experiência', 'Última Instituição Acadêmica', 'Tipo da Formação', 'Formação', 'Duração da Formação', 'Lista de Competências',\n",
    "                               'Lista de Idiomas', 'Lista de Honrarias', 'Quantidade de Recomendações', 'Data de Injestão', 'URL do Perfil'])\n",
    "    \n",
    "    # Definindo um Loop para acessar cada perfil de candidato disponível\n",
    "    for perfil in lista_candidatos:\n",
    "        \n",
    "        # Acessando a URL do perfil do candidato listado\n",
    "        perfil_url = perfil\n",
    "        driver.get(perfil_url)\n",
    "        time.sleep(2) # aguarde 2 segundos para a página abrir\n",
    "        \n",
    "        # Baixando perfil em formato PDF\n",
    "        #baixar_perfil_pdf()\n",
    "        \n",
    "        # Lendo a página principal\n",
    "        codigo_fonte_principal = ler_pagina_principal()\n",
    "        \n",
    "        # Extraindo as informações da seção de Introdução da página\n",
    "        nome_candidato, titulo_intro, localizacao, trabalho_intro, estudo_intro = extrair_introducao(codigo_fonte_principal)\n",
    "        \n",
    "        # Extraindo a quantidade de Seguidores\n",
    "        qtde_seguidores = extrair_seguidores(codigo_fonte_principal)\n",
    "        \n",
    "        # Extraindo a Biografia\n",
    "        biografia = extrair_biografia(codigo_fonte_principal)\n",
    "        \n",
    "        # Obtendo o ID do perfil para acessar as páginas de Experiência, Formação, Idiomas, etc\n",
    "        perfil_id = perfil_url.split('/')[4]\n",
    "        \n",
    "        # Extraindo as informações da seção Experiência Profissional\n",
    "        titulo_experiencia, organizacao_experiencia, tipo_contrato_experiencia, data_inicio_experiencia, data_fim_experiencia, duracao_experiencia = extrair_experiencia_profissional(perfil_id)\n",
    "        \n",
    "        # Extraindo as informações da seção Formação Acadêmica\n",
    "        nome_instituicao, tipo_formacao, nome_formacao, duracao_formacao = extrair_formacao_academica(perfil_id)\n",
    "        \n",
    "        # Extraindo a lista das Competências do perfil\n",
    "        lista_competencias = extrair_competencias(perfil_id)\n",
    "        \n",
    "        # Extraindo a lista com os Idiomas falados pelo candidato\n",
    "        lista_linguas = extrair_idiomas(perfil_id)\n",
    "        \n",
    "        # Extraindo a lista com as Honrarias recebidas pelo candidato\n",
    "        lista_honras = extrair_honrarias(perfil_id)\n",
    "        \n",
    "        # Extraindo a quantidade de Recomendações que o candidato possui\n",
    "        qtde_recomendacoes = extrair_recomendacoes(perfil_id)\n",
    "        \n",
    "        # Obtendo a Data em que o código foi executado\n",
    "        data_ingestao = date.today()\n",
    "        data_ingestao = data_ingestao.strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        # Gerando um DataFrame com as informações\n",
    "        dicionario = {'Nome Candidato': [nome_candidato],\n",
    "                       'Título do Candidato': [titulo_intro],\n",
    "                       'Localização': [localizacao],\n",
    "                       'Organização Atual': [trabalho_intro],\n",
    "                       'Estudou em': [estudo_intro],\n",
    "                       'Quantidade de Seguidores': [qtde_seguidores],\n",
    "                       'Biografia': [biografia],\n",
    "                       'Última Experiência': [organizacao_experiencia],\n",
    "                       'Tipo da Experiência': [tipo_contrato_experiencia],\n",
    "                       'Cargo da Experiência': [titulo_experiencia],\n",
    "                       'Início da Experiência': [data_inicio_experiencia],\n",
    "                       'Fim da Experiência': [data_fim_experiencia],\n",
    "                       'Duração da Experiência': [duracao_experiencia],\n",
    "                       'Última Instituição Acadêmica': [nome_instituicao],\n",
    "                       'Tipo da Formação': [tipo_formacao],\n",
    "                       'Formação': [nome_formacao],\n",
    "                       'Duração da Formação': [duracao_formacao],\n",
    "                       'Lista de Competências': [lista_competencias],\n",
    "                       'Lista de Idiomas': [lista_linguas],\n",
    "                       'Lista de Honrarias': [lista_honras],\n",
    "                       'Quantidade de Recomendações': [qtde_recomendacoes],\n",
    "                       'Data de Injestão': [data_ingestao],\n",
    "                       'URL do Perfil': [perfil]}\n",
    "        \n",
    "        index_dicionario = [0]\n",
    "        \n",
    "        # Convertendo o Dicionário em um DataFrame\n",
    "        df_dicionario = pd.DataFrame(dicionario, index=index_dicionario)\n",
    "\n",
    "        \n",
    "        # Adicionando as informações extraídas dos perfis no DataFrame\n",
    "        df = pd.concat([df, df_dicionario], ignore_index=True)\n",
    "        \n",
    "    # Retornando o DataFrame com os dados extraídos\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13b86094-5140-4d17-ba44-0b809dea725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informando o Caminho do Web Driver, Login, Senha, Lista das URLs dos candidatos\n",
    "login = 'SEU LOGIN'\n",
    "password = 'SUA SENHA'\n",
    "lista_candidatos = ['https://www.linkedin.com/in/williamhgates/',\n",
    "                    'https://www.linkedin.com/in/anitta-larissa-machado-a1118818a/',\n",
    "                    'https://www.linkedin.com/in/crisjunqueira/',\n",
    "                    'https://www.linkedin.com/in/shilpar/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f66bdb4d-a2b3-48df-935b-78376678b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o Web Driver\n",
    "srv = Service('CAMINHO DO WEB DRIVER\\chromedriver.exe')\n",
    "opt = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=srv, options=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05b16e1f-4cfa-4266-9929-4c6a28ad62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando o script\n",
    "df_dados = executar_webscraping(login, password, lista_candidatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a14cb79-4fde-4c9d-9513-867cabd90161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome Candidato</th>\n",
       "      <th>Título do Candidato</th>\n",
       "      <th>Localização</th>\n",
       "      <th>Organização Atual</th>\n",
       "      <th>Estudou em</th>\n",
       "      <th>Quantidade de Seguidores</th>\n",
       "      <th>Biografia</th>\n",
       "      <th>Última Experiência</th>\n",
       "      <th>Tipo da Experiência</th>\n",
       "      <th>Cargo da Experiência</th>\n",
       "      <th>Início da Experiência</th>\n",
       "      <th>Fim da Experiência</th>\n",
       "      <th>Duração da Experiência</th>\n",
       "      <th>Última Instituição Acadêmica</th>\n",
       "      <th>Tipo da Formação</th>\n",
       "      <th>Formação</th>\n",
       "      <th>Duração da Formação</th>\n",
       "      <th>Lista de Competências</th>\n",
       "      <th>Lista de Idiomas</th>\n",
       "      <th>Lista de Honrarias</th>\n",
       "      <th>Quantidade de Recomendações</th>\n",
       "      <th>Data de Injestão</th>\n",
       "      <th>URL do Perfil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>Co-chair, Bill &amp; Melinda Gates Foundation</td>\n",
       "      <td>Seattle, Washington, Estados Unidos</td>\n",
       "      <td>Bill &amp; Melinda Gates Foundation</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>36.033.134 seguidores</td>\n",
       "      <td>Co-chair of the Bill &amp; Melinda Gates Foundatio...</td>\n",
       "      <td>Bill &amp; Melinda Gates Foundation</td>\n",
       "      <td></td>\n",
       "      <td>Co-chair</td>\n",
       "      <td>2000</td>\n",
       "      <td>o momento</td>\n",
       "      <td>22 anos 11 meses</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1973 - 1975</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>16/11/2022</td>\n",
       "      <td>https://www.linkedin.com/in/williamhgates/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anitta Larissa Machado</td>\n",
       "      <td>Embaixadora Global do Nubank</td>\n",
       "      <td>Rio de Janeiro, Rio de Janeiro, Brasil</td>\n",
       "      <td>Estácio</td>\n",
       "      <td></td>\n",
       "      <td>235.346 seguidores</td>\n",
       "      <td>Em 2013, Anitta firmou seu nome no cenário mus...</td>\n",
       "      <td>Estácio</td>\n",
       "      <td></td>\n",
       "      <td>Criadora do Curso Anitta Prepara</td>\n",
       "      <td>mai de 2022</td>\n",
       "      <td>o momento</td>\n",
       "      <td>7 meses</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Cantora, Spanish, Marketing, English, Portugu...</td>\n",
       "      <td>[English, Portuguese, Spanish]</td>\n",
       "      <td>[Prêmio ASCAP Latin Music Awards (jan de 2019)...</td>\n",
       "      <td>0</td>\n",
       "      <td>16/11/2022</td>\n",
       "      <td>https://www.linkedin.com/in/anitta-larissa-mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cristina Junqueira</td>\n",
       "      <td>Co-Founder at Nubank</td>\n",
       "      <td>São Paulo, São Paulo, Brasil</td>\n",
       "      <td>Nubank</td>\n",
       "      <td>Northwestern University - Kellogg School of Ma...</td>\n",
       "      <td>505.612 seguidores</td>\n",
       "      <td>Selected to One Year Accelerated ProgramMajors...</td>\n",
       "      <td>Nubank</td>\n",
       "      <td></td>\n",
       "      <td>Co-Founder</td>\n",
       "      <td>mai de 2013</td>\n",
       "      <td>o momento</td>\n",
       "      <td>9 anos 7 meses</td>\n",
       "      <td>Northwestern University - Kellogg School of Ma...</td>\n",
       "      <td>MBA</td>\n",
       "      <td>Business</td>\n",
       "      <td>2007 - 2008</td>\n",
       "      <td>[Business Strategy, Strategy, Business Plannin...</td>\n",
       "      <td>[English (Nível avançado), Portuguese (Fluente...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>16/11/2022</td>\n",
       "      <td>https://www.linkedin.com/in/crisjunqueira/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shilpa Ranganathan</td>\n",
       "      <td>Corporate Vice President, Windows at Microsoft</td>\n",
       "      <td>Seattle e Região</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Birla Institute of Technology and Science, Pilani</td>\n",
       "      <td>2.153 seguidores</td>\n",
       "      <td>Specialties: Program Management, Project Manag...</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>14 a 7 m</td>\n",
       "      <td>Corporate Vice President, Windows</td>\n",
       "      <td>abr de 2022</td>\n",
       "      <td>o momento</td>\n",
       "      <td>8 meses</td>\n",
       "      <td>Birla Institute of Technology and Science, Pilani</td>\n",
       "      <td>B.E</td>\n",
       "      <td>Electrical &amp; Electronics</td>\n",
       "      <td>1995 - 1999</td>\n",
       "      <td>[Software Development, Program Management, Sof...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>16/11/2022</td>\n",
       "      <td>https://www.linkedin.com/in/shilpar/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Nome Candidato                             Título do Candidato  \\\n",
       "0              Bill Gates       Co-chair, Bill & Melinda Gates Foundation   \n",
       "1  Anitta Larissa Machado                    Embaixadora Global do Nubank   \n",
       "2      Cristina Junqueira                            Co-Founder at Nubank   \n",
       "3      Shilpa Ranganathan  Corporate Vice President, Windows at Microsoft   \n",
       "\n",
       "                              Localização                Organização Atual  \\\n",
       "0     Seattle, Washington, Estados Unidos  Bill & Melinda Gates Foundation   \n",
       "1  Rio de Janeiro, Rio de Janeiro, Brasil                          Estácio   \n",
       "2            São Paulo, São Paulo, Brasil                           Nubank   \n",
       "3                        Seattle e Região                        Microsoft   \n",
       "\n",
       "                                          Estudou em Quantidade de Seguidores  \\\n",
       "0                                 Harvard University    36.033.134 seguidores   \n",
       "1                                                          235.346 seguidores   \n",
       "2  Northwestern University - Kellogg School of Ma...       505.612 seguidores   \n",
       "3  Birla Institute of Technology and Science, Pilani         2.153 seguidores   \n",
       "\n",
       "                                           Biografia  \\\n",
       "0  Co-chair of the Bill & Melinda Gates Foundatio...   \n",
       "1  Em 2013, Anitta firmou seu nome no cenário mus...   \n",
       "2  Selected to One Year Accelerated ProgramMajors...   \n",
       "3  Specialties: Program Management, Project Manag...   \n",
       "\n",
       "                Última Experiência Tipo da Experiência  \\\n",
       "0  Bill & Melinda Gates Foundation                       \n",
       "1                          Estácio                       \n",
       "2                           Nubank                       \n",
       "3                        Microsoft            14 a 7 m   \n",
       "\n",
       "                Cargo da Experiência Início da Experiência Fim da Experiência  \\\n",
       "0                           Co-chair                  2000          o momento   \n",
       "1   Criadora do Curso Anitta Prepara           mai de 2022          o momento   \n",
       "2                         Co-Founder           mai de 2013          o momento   \n",
       "3  Corporate Vice President, Windows           abr de 2022          o momento   \n",
       "\n",
       "  Duração da Experiência                       Última Instituição Acadêmica  \\\n",
       "0       22 anos 11 meses                                 Harvard University   \n",
       "1                7 meses                                                      \n",
       "2         9 anos 7 meses  Northwestern University - Kellogg School of Ma...   \n",
       "3                8 meses  Birla Institute of Technology and Science, Pilani   \n",
       "\n",
       "  Tipo da Formação                  Formação Duração da Formação  \\\n",
       "0                                                    1973 - 1975   \n",
       "1                                                                  \n",
       "2              MBA                  Business         2007 - 2008   \n",
       "3              B.E  Electrical & Electronics         1995 - 1999   \n",
       "\n",
       "                               Lista de Competências  \\\n",
       "0                                                      \n",
       "1  [Cantora, Spanish, Marketing, English, Portugu...   \n",
       "2  [Business Strategy, Strategy, Business Plannin...   \n",
       "3  [Software Development, Program Management, Sof...   \n",
       "\n",
       "                                    Lista de Idiomas  \\\n",
       "0                                                      \n",
       "1                     [English, Portuguese, Spanish]   \n",
       "2  [English (Nível avançado), Portuguese (Fluente...   \n",
       "3                                                      \n",
       "\n",
       "                                  Lista de Honrarias  \\\n",
       "0                                                      \n",
       "1  [Prêmio ASCAP Latin Music Awards (jan de 2019)...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "\n",
       "  Quantidade de Recomendações Data de Injestão  \\\n",
       "0                           0       16/11/2022   \n",
       "1                           0       16/11/2022   \n",
       "2                           0       16/11/2022   \n",
       "3                           4       16/11/2022   \n",
       "\n",
       "                                       URL do Perfil  \n",
       "0         https://www.linkedin.com/in/williamhgates/  \n",
       "1  https://www.linkedin.com/in/anitta-larissa-mac...  \n",
       "2         https://www.linkedin.com/in/crisjunqueira/  \n",
       "3               https://www.linkedin.com/in/shilpar/  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibindo o resultado\n",
    "pd.set_option('max_columns', None)\n",
    "df_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9668080f-74e8-4759-9644-d568c4a632a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os Resultados em uma Planilha Excel\n",
    "df_dados.to_excel(\"Resultados Web Scraping.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169389b-bbec-4ab6-92fa-15ac9b83b65c",
   "metadata": {},
   "source": [
    "Pelo DataFrame acima é possível ver que a extração foi um sucesso, lembrando que os campos em branco são devido à ausência da informação pelos próprios usuários e a coluna `Tipo da Experiência` precisa passar por um tratamento, uma vez que pela estrutura da página acaba sendo extraída a duração geral da experiência de forma errada, como vemos na quarta linha do DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25734a-3d28-47ff-93dd-5f9ef883b409",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Com o Script desenvolvido neste projeto concluímos com sucesso nosso objetivo de levar as informações dos candidatos das vagas de emprego para uma única tabela, que contém os dados relevantes para a tomada de decisão do time de Recursos Humanos para selecionar os candidatos mais qualificados.\n",
    "\n",
    "Como o propósito do projeto era apenas apresentar o funcionamento do Web Scraping para extrair dados valiosos do LinkedIn, não irei explorar as aplicações que poderiam ser feitas com os dados obtidos como resultado do projeto, mas deixo uma provocação: \n",
    "\n",
    "***Quais seriam os impactos dessas informações extraídas em um Negócio quando utilizadas em conjunto com modelos de Inteligência Artificial? Direcionamento de Marketing? Recrutamento? Prevenção? Recomendação? Etc.***\n",
    "\n",
    "Eu sei que provavelmente você já deve ter tido várias ideias de como usar essas informações ao longo desse projeto e agora com essa provocação podem ter surgido ainda mais ideias brilhantes, porém é preciso ter em mente que a Plataforma LinkedIn sabe o quão valiosas essas informações são e por isso vem dificultado cada vez mais a extração via Scraping. Portanto, hoje o script desenvolvido pode funcionar perfeitamente, mas amanhã pode ser que nada mais possa ser aproveitado do processo.\n",
    "\n",
    "Dito isso, é importante relembrar o que foi dito no início do projeto, a Plataforma vem tomando medidas para detectar o uso de bots ou qualquer processo de extração de dados de seus usuários, mesmo estes dados sendo públicos, então tome cuidado e não cometa abusos na Plataforma com qualquer aplicação que seja feita. Embora o Web Scraping não seja o ideal e nem viável para escalar soluções ou produtizar aplicações, a própria Plataforma do LinkedIn possui uma [API Oficial](https://learn.microsoft.com/en-us/linkedin/), que desde 2015 deixou de ser pública (por conta do valor das informações), que entrega informações além das obtidas com o Web Scraping desenvolvido, sendo necessário criar uma aplicação na Plataforma e também se tornar [Parceiro](https://developer.linkedin.com/content/developer/global/en_us/index/partner-programs/apply), onde irá passar por uma seleção burocrática e, mesmo se aprovado, ainda podem haver limitações ou até mesmo planos de assinatura para obter as informações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd81a-42f2-4529-baf7-4f91cf17e30a",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "O que é o Web Scraping:\n",
    "* https://canaltech.com.br/seguranca/o-que-e-web-scraping/\n",
    "* https://pplware.sapo.pt/internet/web-scraping-saiba-o-que-e-e-para-que-serve/\n",
    "\n",
    "Luta do LinkedIn para proibir o Web Scraping:\n",
    "* https://olhardigital.com.br/2021/06/14/internet-e-redes-sociais/linkedin-ganhar-nova-chance-para-tentar-impedir-coleta-de-dados-publicos-por-rival/\n",
    "* https://www.natlawreview.com/article/hiq-labs-v-linkedin\n",
    "* https://www.csoonline.com/article/3662039/hiq-v-linkedin-court-ruling-will-have-a-material-effect-on-privacy.html\n",
    "* https://www.adweek.com/media/court-sides-with-linkedin-in-data-scraping-lawsuit-vs-hiq-labs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
